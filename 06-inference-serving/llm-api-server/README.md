# LLM Inference Serving & Deployment

This project serves fine-tuned LLM models behind a production-style REST API using FastAPI.

## Components

- Model loading
- Tokenized inference
- FastAPI server deployment
- Client testing interface

## Future Work

- GPU deployment on cloud
- vLLM integration for high-speed serving
- Docker containerization

