# Mischa Emerson â€” LLM Research Engineer Portfolio

Welcome to my applied AI research engineering portfolio. This repository contains my growing body of hands-on work focused on Large Language Model (LLM) systems, covering:

- âœ… LLM fine-tuning and parameter-efficient training (LoRA, PEFT)
- âœ… Data preparation and cleaning for model training
- âœ… Evaluation frameworks for LLM output quality (BLEU, ROUGE, Exact Match)
- âœ… Responsible AI benchmarking (toxicity, bias, safety)
- âœ… Distributed multi-GPU training experiments (Accelerate, DDP)
- âœ… Efficient inference serving (FastAPI API deployment)
- âœ… RLHF simulation framework (Phase 07)
- âœ… Prompt engineering & reasoning experiments (Phase 08)
- âœ… Live deployment prep (Phase 09 Hyperpolish)

---

## ðŸ“‚ Project Directory

| Phase | Project | Status |
|---|---|---|
| 00 | Foundations & Hugging Face Prep | âœ… Complete |
| 01 | Fine-Tuning Small LLMs (Phi-2, TinyLlama, Mistral) | âœ… Complete |
| 02 | Data Cleaning Pipelines for LLM Datasets | âœ… Complete |
| 03 | Evaluation Frameworks (BLEU, ROUGE, EM) | âœ… Complete |
| 04 | Bias & Responsible AI Testing (toxicity, bias) | âœ… Complete |
| 05 | Distributed Training with DDP & Accelerate | âœ… Complete |
| 06 | Inference Serving & Deployment (FastAPI, Transformers) | âœ… Complete |
| 07 | RLHF Simulation Prep | âœ… Complete |
| 08 | Prompt Engineering & COT Experiments | ðŸ”¨ In Progress |
| 09 | Hyperpolish Pack (Demo Hosting, Docker, Deployment) | ðŸ”œ Launching |

---

## ðŸ”§ Phase 07 Highlight â€“ RLHF Simulation Prep

This phase simulates the data preparation process used in RLHF pipelines. I created a structured JSONL dataset of prompt/completion pairs with human preference labels, then wrote a script to convert it into a binary-labeled format suitable for training reward models. This simulates the "pre-reward model" phase of modern alignment pipelines.

**Files Created:**
- `rlhf_prefs.jsonl` â€“ raw preferences  
- `prep_rlhf_dataset.py` â€“ Python script to convert to reward data  
- `rlhf_reward_data.csv` â€“ labeled output for supervised reward training

Run the script:
```bash
python3 prep_rlhf_dataset.py

---

Preview of output dataset:

| prompt                         | completion      | label |
|--------------------------------|-----------------|-------|
| What is the capital of France? | Paris           | 1     |
| What is the capital of France? | London          | 0     |
| What is 2 + 2?                 | 4               | 1     |
| What is 2 + 2?                 | 22              | 0     |
| Who wrote 1984?                | George Orwell   | 1     |
| Who wrote 1984?                | J.K. Rowling    | 0     |

This step taught me how feedback data is encoded into usable labels and how reward modeling fits into real-world GenAI workflows.

---

## ðŸš€ Summary

This portfolio was designed to demonstrate full-stack LLM research engineering capabilities in preparation for roles including:

- Research Engineer @ Meta AI (FAIR)
- Applied LLM Engineer @ OpenAI / Anthropic / Google DeepMind / Hugging Face
- Responsible AI Engineer
- Applied AI Researcher

All projects are fully reproducible and documented with real-world industry pipelines.

---

## ðŸ”— Connect

- [LinkedIn Profile](https://www.linkedin.com/in/mischaemerson)
- [Portfolio: Mischaâ€™s Notion](https://merciful-ginger-7b6.notion.site/Mischa-Emerson-Engeering-Portfolio-1ff8807f8f868051bedac07eae351154)

- [Hugging Face Profile](mischaemerson)

---

Actively Building. Actively Learning. Open to collaboration. ðŸš€

