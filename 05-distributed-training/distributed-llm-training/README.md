# Distributed Fine-Tuning Pipeline

This project demonstrates distributed LLM fine-tuning pipelines using:

- PyTorch Distributed Data Parallel (DDP)
- Hugging Face Accelerate
- Cloud GPU and Colab Pro+ setups

## Key Features

- Multi-GPU support
- LoRA parameter-efficient fine-tuning
- Train/test-ready for larger models (Phi-2, Mistral-7B, etc.)
- Fully reproducible config system

## Future Work

- Add AWS Sagemaker support
- Add Azure ML integration

